{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c20b067-733c-4a76-a023-55e4019e200e",
   "metadata": {},
   "source": [
    "Import des packages utilisés pour ce projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e319c-aeab-4562-a995-9335ae2f23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal\n",
    "import pickle\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import random\n",
    "import shutil # pour copier coller les fichiers de clouds vers organized_clouds\n",
    "import cv2\n",
    "from imgaug import augmenters as iaa\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import tensorflow_addons as tfa # nécessite version tensorflow antèrieur : pip install tensorflow==2.13.0\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img # pour preprocessing img et plot img validation\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Model # pour compilation model\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split # pour validation model\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score, roc_curve, auc, f1_score, precision_score, recall_score # pour evaluation prédiction model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# from kerastuner import HyperParameters \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fdd75-2294-4368-bcf5-8c7c3fc77500",
   "metadata": {},
   "source": [
    "Paramètres de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867bbba-6678-40f2-8e21-0de81a4c0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemins d'accès\n",
    "data_dir = \"C:/Users/33619/Desktop/Master_MODE/M2/Machine learning/data/\" # directory for images\n",
    "organized_dir = \"C:/Users/33619/Desktop/Master_MODE/M2/Machine learning/data/organized_clouds\" # organized directory\n",
    "augmented_dir = \"C:/Users/33619/Desktop/Master_MODE/M2/Machine learning/data/augmented_clouds\" # dataset organisé et augmenté\n",
    "save_mod_dir = \"C:/Users/33619/Desktop/Master_MODE/M2/Machine learning/data/models_trained/\"\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "# Vérifier si le fichier ZIP existe et si le dossier de destination n'existe pas\n",
    "if os.path.isfile(data_dir + \"clouds.zip\") and not os.path.isdir(data_dir + \"clouds\"):\n",
    "    print('unzip')\n",
    "    # Extraire le fichier ZIP\n",
    "    with zipfile.ZipFile(data_dir + \"clouds.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir + \"clouds\")  # Extraire dans un répertoire \"clouds\"\n",
    "else:\n",
    "    print(\"data directory already ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7d409-0930-459b-a2c1-9171fdba2ed5",
   "metadata": {},
   "source": [
    "Paramètres des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878a5a2-b3a8-4398-ba76-48cedea58850",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "nb_class = 4 # clear / partly couldy / couldy / haze\n",
    "class_names = [\"clear\",\"partly_cloudy\",\"cloudy\",\"haze\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a86a0-659d-4ac3-a7f1-40054f1822b5",
   "metadata": {},
   "source": [
    "Hyperparamètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec367c-510b-42d3-85ad-dda9650de266",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_mod = 64 # nb d'échantillons traités ensembles. Après avoir traité tout les lots = une époch complète \n",
    "epoch_mod = 20 # nb de fois où les input sont pris en compte\n",
    "dropout_mod = 0.1\n",
    "learning_rate_mod = 0.0001\n",
    "taille_lot_augm_train = 6000\n",
    "taille_lot_augm_valid = 1000\n",
    "\n",
    "# paramètres architecture model à ajouter\n",
    "nb_filtre_1, taille_filtre_1 = 16, (5,5) # pour couche convolution 1\n",
    "nb_filtre_2, taille_filtre_2 = 128, (3,3) # pour couche convolution 2\n",
    "nb_filtre_3, taille_filtre_3 = 256, (3,3) # pour couche convolution 3\n",
    "taille_pool_size = (2,2) # ici chaque passage réduit la hauteur et la largeur de moitié\n",
    "nb_neurones_dense = 64 # pour couche dense 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49244255-6d76-4aa4-9c9d-48eac355634e",
   "metadata": {},
   "source": [
    "##Création des fichiers images + traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823f655-c702-4ce5-b19d-9ea4c4bda064",
   "metadata": {},
   "source": [
    "Comptage des fichiers dans les sous dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cb6c8-76de-402b-958d-0eeb4f777483",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(data_dir + \"clouds\", class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        file_count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
    "        print(f\"TOTAL : Classe {class_folder}: {file_count} images\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675a26b-f360-4178-88bd-e55c048cdd23",
   "metadata": {},
   "source": [
    "Séparation des données train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2ca1d-a939-48b5-81c4-57bc5e5e68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(organized_dir):\n",
    "    user_response = input(f\"Voulez-vous créer/remplacer '{organized_dir}'? (oui/non): \").strip().lower()\n",
    "    if user_response == \"oui\":\n",
    "        replace_organized_folder = True\n",
    "    else:\n",
    "        replace_organized_folder = False\n",
    "else:\n",
    "    replace_organized_folder = True\n",
    "\n",
    "# si dossier organized_clouds à remplacer ou recréer:\n",
    "if replace_organized_folder:\n",
    "    if os.path.exists(organized_dir):\n",
    "        shutil.rmtree(organized_dir)  # supprime le dossier organized_clouds si il existe\n",
    "        print(f\"Suppression du dossier {organized_dir}\")\n",
    "        \n",
    "    # Création des dossiers train, test, et validation\n",
    "    os.makedirs(os.path.join(organized_dir, 'train'), exist_ok=True)  # Dossier pour train\n",
    "    os.makedirs(os.path.join(organized_dir, 'test'), exist_ok=True)   # Dossier pour test\n",
    "    os.makedirs(os.path.join(organized_dir, 'valid'), exist_ok=True)  # Dossier pour validation\n",
    "    \n",
    "    # Ratios pour les splits\n",
    "    train_ratio = 0.7  # 70% pour train\n",
    "    valid_ratio = 0.15  # 15% pour validation\n",
    "    test_ratio = 0.15  # 15% pour test\n",
    "    \n",
    "    # Boucle pour séparer les images dans chaque classe\n",
    "    for class_name in class_names:\n",
    "        print(class_name)\n",
    "        class_path = os.path.join(data_dir + \"clouds\", class_name)  # Chemin vers le dossier de classe actuel\n",
    "        images = os.listdir(class_path)  # Liste de toutes les images du dossier\n",
    "        \n",
    "        # Vérification de la présence d'images dans le dossier\n",
    "        if len(images) == 0:\n",
    "            print(f\"Pas d'image trouvée dans le dossier de la classe {class_path}. Saut de cette classe.\")\n",
    "            continue  # Passe à la classe suivante s'il n'y a pas d'images\n",
    "            \n",
    "        # Séparation des images en trois ensembles : train, validation, et test\n",
    "        train_images, valid_test_images = train_test_split(images, train_size=train_ratio) # random_state=random_seed\n",
    "        valid_images, test_images = train_test_split(valid_test_images, test_size=test_ratio/(valid_ratio + test_ratio)) # random_state=random_seed\n",
    "        \n",
    "        # Création des sous-dossiers pour chaque classe dans train, validation et test\n",
    "        os.makedirs(os.path.join(organized_dir, 'train', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(organized_dir, 'valid', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(organized_dir, 'test', class_name), exist_ok=True)\n",
    "\n",
    "        # Copier les images dans les dossiers correspondants\n",
    "        for img in train_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(organized_dir, 'train', class_name, img))\n",
    "        for img in valid_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(organized_dir, 'valid', class_name, img))\n",
    "        for img in test_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(organized_dir, 'test', class_name, img))\n",
    "\n",
    "    print(f\"Le dossier {organized_dir} a été remplacé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ae7ec-6239-49bc-865e-fcc6d36ccc94",
   "metadata": {},
   "source": [
    "Création des dossiers d'images augmentés artificiellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d702a3-09b4-41d0-a60b-cbcf04cd02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(augmented_dir):\n",
    "    user_response = input(f\"Voulez-vous créer/remplacer '{augmented_dir}'? (oui/non): \").strip().lower()\n",
    "    replace_augmented_folder = user_response == \"oui\"\n",
    "else:\n",
    "    replace_augmented_folder = True\n",
    "\n",
    "### Augmentation des images pour équilibrer nb d'indivs par classe et apporter variabilité dans les données d'entraînement et validation\n",
    "\n",
    "def augmenter_images(class_path, target_count):\n",
    "    augmenter = iaa.Sequential([\n",
    "        iaa.Resize((img_height, img_width)), \n",
    "        iaa.Fliplr(0.5),  # flip vertical proba 0.5\n",
    "        iaa.Flipud(0.5), #\" flip horizontal proba 0.5\n",
    "        # iaa.Affine(rotate=(-15, 15)), # rotation angulaire possible, mais ajoute barres noires\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)), # ajout bruit\n",
    "        iaa.Multiply((0.8, 1.2)), # ajout luminosité\n",
    "        iaa.LinearContrast((0.8, 1.2)), # ajout contraste\n",
    "    ])\n",
    "    \n",
    "    images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))] # /!\\ capter comment ça choisit les images\n",
    "    num_images = len(images)\n",
    "    \n",
    "    print(\"augmentation de chaque classe pour respectivement train et valid\")\n",
    "    \n",
    "    if num_images >= target_count:\n",
    "        print(f\"Classe '{os.path.basename(class_path)}' a déjà {num_images} images. Pas besoin d'augmentation.\")\n",
    "        # Supprime les images en excès si elles dépassent la limite cible\n",
    "        excess_images = images[target_count:]  # Sélectionne les images au-delà de la limite\n",
    "        for img_name in excess_images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            os.remove(img_path)\n",
    "        print(f\"Classe '{os.path.basename(class_path)}' a maintenant {target_count} images après suppression.\")\n",
    "        return\n",
    "    \n",
    "    while len(images) < target_count:\n",
    "        for img_name in images:\n",
    "            if len(images) >= target_count:\n",
    "                break\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")  # Convertir en RGB pour compatibilité JPEG\n",
    "            image = np.array(image)\n",
    "            augmented_image = augmenter(image=image)\n",
    "            augmented_img = Image.fromarray(augmented_image)\n",
    "            \n",
    "            # Nommer les images de manière concise\n",
    "            augmented_img_name = f\"aug_{len(images)}_{os.path.basename(img_name)}\"\n",
    "            augmented_img_path = os.path.join(class_path, augmented_img_name)\n",
    "            augmented_img.save(augmented_img_path)\n",
    "            images.append(augmented_img_name)\n",
    "    \n",
    "    print(f\"Classe '{os.path.basename(class_path)}' a maintenant {len(images)} images.\")\n",
    "\n",
    "# Gestion du dossier d'augmentation\n",
    "if replace_augmented_folder:\n",
    "    if os.path.exists(augmented_dir):\n",
    "        shutil.rmtree(augmented_dir)\n",
    "        print(f\"Suppression du dossier {augmented_dir}\")\n",
    "    \n",
    "    # Création des dossiers train, valid et test pour augmentation\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(augmented_dir, 'train', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(augmented_dir, 'valid', class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(augmented_dir, 'test', class_name), exist_ok=True)\n",
    "\n",
    "        # DONNÉES TEST (non augmentées)\n",
    "        class_test_path = os.path.join(organized_dir, 'test', class_name)\n",
    "        images_test = os.listdir(class_test_path)\n",
    "        \n",
    "        if len(images_test) == 0:\n",
    "            print(f\"Pas d'image trouvée dans le dossier test de la classe {class_name}. Saut de cette classe.\")\n",
    "            continue\n",
    "        \n",
    "        # Copier les images de test non augmentées\n",
    "        for img in images_test:\n",
    "            shutil.copy2(os.path.join(class_test_path, img), os.path.join(augmented_dir, 'test', class_name, img))\n",
    "    \n",
    "    # Appliquer aux dossiers d'entraînement et de validation\n",
    "    for class_name in class_names:\n",
    "        train_class_path = os.path.join(augmented_dir, 'train', class_name)\n",
    "        organized_train_class_path = os.path.join(organized_dir, 'train', class_name)\n",
    "        shutil.copytree(organized_train_class_path, train_class_path, dirs_exist_ok=True)\n",
    "        augmenter_images(train_class_path, taille_lot_augm_train)\n",
    "    \n",
    "        valid_class_path = os.path.join(augmented_dir, 'valid', class_name)\n",
    "        organized_valid_class_path = os.path.join(organized_dir, 'valid', class_name)\n",
    "        shutil.copytree(organized_valid_class_path, valid_class_path, dirs_exist_ok=True)\n",
    "        augmenter_images(valid_class_path, taille_lot_augm_valid)\n",
    "        \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddfed3-21bb-4f37-af8c-5b89d38b0022",
   "metadata": {},
   "source": [
    "Vérification du nombre d'images dans chaque ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e08ee-efb2-4e92-a388-d59ddb402e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in class_names:\n",
    "    train_class_path = os.path.join(augmented_dir, 'train', class_name)\n",
    "    num_train_files = len(os.listdir(train_class_path)) if os.path.exists(train_class_path) else 0\n",
    "    print(f\"TRAIN '{class_name}': {num_train_files}\")\n",
    "print(\"\\n\")\n",
    "for class_name in class_names:\n",
    "    valid_class_path = os.path.join(augmented_dir, 'valid', class_name)\n",
    "    num_valid_files = len(os.listdir(valid_class_path)) if os.path.exists(valid_class_path) else 0\n",
    "    print(f\"VALID '{class_name}': {num_valid_files}\")\n",
    "print(\"\\n\")\n",
    "for class_name in class_names:\n",
    "    test_class_path = os.path.join(augmented_dir, 'test', class_name)\n",
    "    num_test_files = len(os.listdir(test_class_path)) if os.path.exists(test_class_path) else 0\n",
    "    print(f\"TEST '{class_name}': {num_test_files}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f0c31-7ed3-41b9-80c9-ce0d53b2005b",
   "metadata": {},
   "source": [
    "Création du générateur d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e35bbe-6f4a-42c1-80c8-7407569ac3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218e8af-4a85-4b4f-bdf3-0501faeedeed",
   "metadata": {},
   "source": [
    "Création des générateur de data d'entrainement, de validation et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ae0d6-167d-406b-8bae-a359a716cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator.flow_from_directory(\n",
    "    data_dir + \"augmented_clouds/train\", # augmented_clouds/train # organized_clouds\n",
    "    target_size = (img_width,img_height),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = batch_size_mod, \n",
    "    class_mode = \"sparse\", # fonction de perte => cross entropy\n",
    "    shuffle = True, #  empeche le modèle d'apprendre sur ordre des échantillons\n",
    "    )\n",
    "\n",
    "\n",
    "# Creation data validation\n",
    "valid_generator = generator.flow_from_directory(\n",
    "    data_dir + \"augmented_clouds/valid\", # augmented_clouds/train # organized_clouds\n",
    "    target_size = (img_width,img_height),\n",
    "    batch_size = batch_size_mod,\n",
    "    class_mode = \"sparse\",\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "# Creation data test\n",
    "test_generator = generator.flow_from_directory(\n",
    "    data_dir + \"augmented_clouds/test\", # augmented_clouds/train\n",
    "    target_size = (256, 256),\n",
    "    batch_size = 1,\n",
    "    class_mode = \"sparse\",\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913658df-53dd-4fa0-8ee0-6df4893ceab6",
   "metadata": {},
   "source": [
    "Test pour voir si les images sont bien labélisées dans chaque générateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722ee85-9dec-4272-976b-8f0398e34ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test = 1\n",
    "type_generator = train_generator\n",
    "# type_generator = valid_generator\n",
    "# type_generator = test_generator\n",
    "\n",
    "for i in range(nb_test):\n",
    "    images, labels = next(type_generator)\n",
    "    \n",
    "    image = images[0]\n",
    "    label = labels[0]\n",
    "    \n",
    "    class_index = np.argmax(label)\n",
    "    \n",
    "    plt.imshow(image)  \n",
    "    plt.axis('off')\n",
    "    plt.title(round(label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174ac12-6068-4597-b995-f04515fec37f",
   "metadata": {},
   "source": [
    "Création de l'architecture du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2850c7d4-3dbf-471b-b42f-dc4b51554bdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Création d'un réseau de neurones vide \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Input \u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(img_width, img_height,\u001b[38;5;241m3\u001b[39m)) \u001b[38;5;66;03m# 3 car RVB\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Création d'un réseau de neurones vide \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Input \n",
    "model_input = Input(shape=(img_width, img_height,3)) # 3 car RVB\n",
    "\n",
    "# 1ère couche : convolution + activation ReLU + max-pooling + Dropout\n",
    "model = Conv2D(nb_filtre_1, taille_filtre_1, padding = \"same\")(model_input)\n",
    "model = Activation(\"relu\")(model)\n",
    "model = MaxPooling2D(pool_size = taille_pool_size)(model) # 2,2 taille par défault = chaque passage réduit la hauteur et la largeur de moitié.\n",
    "#regularisation pr eviter le surapprentissage, permet d'éteindre des neurones à chaque époch. Valeur = % de neurones à éteindre => à mettre entre couche dense et parfois entre couches convolutionelles\n",
    "model = Dropout(dropout_mod)(model) \n",
    "\n",
    "#2ème couche : convolution + activation ReLU + max-pooling + Dropout\n",
    "model = Conv2D(nb_filtre_2, taille_filtre_2, padding = \"same\")(model)\n",
    "model = Activation(\"relu\")(model)\n",
    "model = MaxPooling2D(pool_size = taille_pool_size)(model)\n",
    "model = Dropout(dropout_mod)(model) \n",
    "\n",
    "# 3ème couche : convolution + activation ReLU + max-pooling + Dropout\n",
    "model = Conv2D(nb_filtre_3, taille_filtre_3, padding = \"same\")(model)\n",
    "model = Activation(\"relu\")(model)\n",
    "model = MaxPooling2D(pool_size = taille_pool_size)(model)\n",
    "model = Dropout(dropout_mod)(model) \n",
    "\n",
    "# 4ème couche : applatissement + couche Dense + activation ReLU + Dropout\n",
    "model = Flatten()(model)\n",
    "model = Dense(nb_neurones_dense)(model)\n",
    "model = Activation(\"relu\")(model)\n",
    "model = Dropout(dropout_mod)(model) \n",
    "\n",
    "# Output\n",
    "model = Dense(nb_class)(model)\n",
    "model_output = Activation(\"softmax\")(model)\n",
    "\n",
    "# summary\n",
    "model_final = Model(model_input, model_output)\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054739b-7a28-40f9-9433-2d9036f84e1b",
   "metadata": {},
   "source": [
    "Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff9d8c-824b-4a6a-9c67-43d1e8f9fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILATION AVEC RSMPROP OU ADAM\n",
    "\n",
    "model_final.compile(optimizer = Adam(learning_rate = learning_rate_mod), \n",
    "                    loss = \"sparse_categorical_crossentropy\",\n",
    "                    metrics = [\n",
    "                    'accuracy',\n",
    "                    # tfa.metrics.F1Score(num_classes=num_classes, average='macro'),  # ou 'micro' selon votre besoin\n",
    "                    # tfa.metrics.CohenKappa(),\n",
    "                    # tfa.metrics.Precision(),\n",
    "                    # tfa.metrics.Recall()\n",
    "                    ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9b376-b574-40cf-9283-70539f38160d",
   "metadata": {},
   "source": [
    "###Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86508613-7534-4e30-9d11-d299edec0fe6",
   "metadata": {},
   "source": [
    "Implémentation tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e12d2a-7f8d-4eac-b351-ef098978de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    " import time\n",
    " run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    " return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "#### --------------------------------------------------------------------------\n",
    "\n",
    "# Lancer TensorBoard automatiquement\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38396d3-4285-49db-bdbf-e54301e4a974",
   "metadata": {},
   "source": [
    "Option de réduction d'apprentissage quand il n'y a plus de progrès"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad03c0-75fa-43b4-8a0f-93aa358982bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342db2d3-7611-40e5-8b20-8452d719e4b7",
   "metadata": {},
   "source": [
    "Récap avant l'entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7e698-66bc-413d-a827-8bedd52f698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nEntrainement du model sur {taille_lot_augm_train} images par classes sur {epoch_mod} epochs composé de batch de {batch_size_mod} images \\ntaux d'apprentissage initial = {learning_rate_mod}\\nDropout = {dropout_mod}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301d211-b5ea-4750-bd1f-546bceef98ab",
   "metadata": {},
   "source": [
    "Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cc692-7bfc-4f7c-82d5-386ea5709a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_final.fit(\n",
    "              train_generator,\n",
    "              # class_weight = class_weights_mod, # <===================== ne marche pas si activé \n",
    "              epochs = epoch_mod,\n",
    "              validation_data = valid_generator,\n",
    "              callbacks=[tensorboard_cb,lr_scheduler] # tensorboard_cb pour appel tensorboard. On peut aussi ajouter lr_scheduler. # checkpoints ??\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055bdbf7-df7d-49a1-85b8-b057d33e08d9",
   "metadata": {},
   "source": [
    "Sauvegarde du modèle entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c40672-d826-4faa-9724-b363526427b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.now().strftime(\"%d_%m_%Hh%M\")  # JJ_MM\n",
    "model_final.save(save_mod_dir + f\"cloud_classifier_model_{date_str}_avec_{epoch_mod}_epochs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c91aa3-ff63-486c-9570-e285932ed5b7",
   "metadata": {},
   "source": [
    "Sauvegarde de l'historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb8a93-b998-4299-80fa-d14e020db54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c56d55-e89b-4b8e-84dd-4f2f6b18668a",
   "metadata": {},
   "source": [
    "Validation du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc33fd-35c7-44f6-b470-3786d21602ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION sur des données de validation \n",
    "\n",
    "# Evolution accuracy => historique du modèle\n",
    "# capacité à bien prédire qui évolue\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='valid accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evolution Fonction de perte (CE) => historique du modèle\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evolution learning_rate => historique du modèle\n",
    "plt.plot(history.history['learning_rate'], label='learning_rate')\n",
    "plt.title('learning_rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5239b-e0c9-4b94-a9f6-3c881cadd62c",
   "metadata": {},
   "source": [
    "Prédiction et évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c57207-1681-455e-a29b-8a6b1f1a3973",
   "metadata": {},
   "source": [
    "Création d'un échantillon équilibré du dataset pour tester l'efficacité du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48a8d4-f935-4a80-a3ca-687c1cabbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_test = True\n",
    "if little_test:\n",
    "    # possibilité de réaliser les test sur un ensemble + petit pour équilibrer les classe lors de l'interprétation\n",
    "    test_dir = \"C:/Users/User/Desktop/MASTER/M2/MLB/PROJET/organized_clouds/test\"  # Dossier d'origine\n",
    "    little_test_dir = \"C:/Users/User/Desktop/MASTER/M2/MLB/PROJET/little_test\"  # Nouveau dossier\n",
    "    max_images = 314 # pour cloudy = 314\n",
    "    \n",
    "    if os.path.exists(little_test_dir):\n",
    "        user_response = input(f\"Le dossier '{little_test_dir}' existe déjà. Supprimer le ! ? (oui/non): \").strip().lower()\n",
    "        if user_response == \"oui\":\n",
    "            shutil.rmtree(little_test_dir)\n",
    "            print(f\"Le dossier '{little_test_dir}' a été supprimé.\")\n",
    "        else:\n",
    "            print(\"Opération annulée.\") # Arrête le script si l'utilisateur choisit de ne pas remplacer\n",
    "    else:\n",
    "        print(f\"Création du dossier '{little_test_dir}'.\")\n",
    "    \n",
    "    # Création du dossier `little_test` et sous-dossiers pour chaque classe\n",
    "    os.makedirs(little_test_dir, exist_ok=True)\n",
    "    \n",
    "    # Copie des images dans `little_test` en respectant la limite\n",
    "    for class_name in class_names:\n",
    "        class_test_path = os.path.join(test_dir, class_name)\n",
    "        class_little_test_path = os.path.join(little_test_dir, class_name)\n",
    "        os.makedirs(class_little_test_path, exist_ok=True)\n",
    "        \n",
    "        # Récupérer toutes les images de la classe et en choisir aléatoirement un sous-ensemble\n",
    "        images = [img for img in os.listdir(class_test_path) if os.path.isfile(os.path.join(class_test_path, img))]\n",
    "        selected_images = random.sample(images, min(len(images), max_images))\n",
    "        \n",
    "        # Copier les images sélectionnées\n",
    "        for img in selected_images:\n",
    "            src_path = os.path.join(class_test_path, img)\n",
    "            dst_path = os.path.join(class_little_test_path, img)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "    \n",
    "    little_test_generator = generator.flow_from_directory(\n",
    "        little_test_dir,\n",
    "        target_size = (256, 256),\n",
    "        batch_size = 1,\n",
    "        class_mode = \"sparse\",\n",
    "        shuffle = False\n",
    "    )\n",
    "\n",
    "if little_test:\n",
    "    test_generator = little_test_generator\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        train_class_path = os.path.join(little_test_dir, class_name)\n",
    "        num_train_files = len(os.listdir(train_class_path))\n",
    "        print(f\"TEST '{class_name}': {num_train_files}\")\n",
    "\n",
    "else:\n",
    "    test_generator = generator.flow_from_directory(\n",
    "        data_dir + \"augmented_clouds/test\", # augmented_clouds/train\n",
    "        target_size = (256, 256),\n",
    "        batch_size = 1,\n",
    "        class_mode = \"sparse\",\n",
    "        shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47ebc5-d444-4c9f-8208-5daecc3f5ad8",
   "metadata": {},
   "source": [
    "Prédiction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac293a24-cab5-42e4-ac2b-803f6713b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_prob = model_final.predict(test_generator)\n",
    "y_test_pred_classes = np.argmax(y_test_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f3863-f0bb-45db-b359-63615cd31a08",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2399f56-cacf-497d-9e9c-4e2fc9dfeba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_final.evaluate(test_generator)\n",
    "print(f\"Test loss : {test_loss}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c50d7-d7a5-4f30-9a48-6d3c43db7cbe",
   "metadata": {},
   "source": [
    "Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c5f9d-2d07-45c6-b8c9-87e2ed19a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = test_generator.classes  # Récupérer vrais labels\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_true, y_test_pred_classes)\n",
    "print(conf_matrix) #### attention biaisé par le grand nombre d'images dans clear ! \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greys', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Classes Réelles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736c46f-b06f-493f-b21d-f820bf86c8d5",
   "metadata": {},
   "source": [
    "Matrice de confusion pour chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72415b55-e54f-4776-a516-0128c9451ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_class):\n",
    "    TP = conf_matrix[i, i]  # Vrais positifs\n",
    "    FP = conf_matrix[:, i].sum() - TP  # Faux positifs\n",
    "    FN = conf_matrix[i, :].sum() - TP  # Faux négatifs\n",
    "    TN = conf_matrix.sum() - (FP + FN + TP)  # Vrais négatifs\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f\"Classe {class_names[i]} :\")\n",
    "    print(f\"  Vrais Positifs (TP): {TP}\")\n",
    "    print(f\"  Faux Positifs (FP): {FP}\")\n",
    "    print(f\"  Faux Négatifs (FN): {FN}\")\n",
    "    print(f\"  Vrais Négatifs (TN): {TN}\")\n",
    "    print(\" \")\n",
    "\n",
    "for i in range(nb_class):\n",
    "    # Créer une matrice de confusion pour chaque classe\n",
    "    class_conf_matrix = np.zeros((2, 2), dtype=int)\n",
    "    class_conf_matrix[0, 0] = conf_matrix[i, i]  # TP\n",
    "    class_conf_matrix[0, 1] = conf_matrix[:, i].sum() - class_conf_matrix[0, 0]  # FP\n",
    "    class_conf_matrix[1, 0] = conf_matrix[i, :].sum() - class_conf_matrix[0, 0]  # FN\n",
    "    class_conf_matrix[1, 1] = conf_matrix.sum() - (class_conf_matrix[0, 1] + class_conf_matrix[1, 0] + class_conf_matrix[0, 0])  # TN\n",
    "\n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(class_conf_matrix, annot=True, fmt='d', cmap='Greys', \n",
    "                xticklabels=['Prédit Positif', 'Prédit Négatif'], \n",
    "                yticklabels=['Réel Positif', 'Réel Négatif'])\n",
    "    plt.title(f'Matrice de Confusion pour la Classe : {class_names[i]}')\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Véritables Étiquettes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddd28b-3132-4655-abdb-fb0ea3bbb13e",
   "metadata": {},
   "source": [
    "Précision/Rappel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4164337-f629-458a-9868-a76c5879b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_per_class = []\n",
    "recall_per_class = []\n",
    "\n",
    "for i in range(nb_class):\n",
    "    TP = conf_matrix[i, i]  # Vrais positifs\n",
    "    FP = conf_matrix[:, i].sum() - TP  # Faux positifs\n",
    "    FN = conf_matrix[i, :].sum() - TP  # Faux négatifs\n",
    "    \n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Précision\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0      # Rappel\n",
    "    \n",
    "    precision_per_class.append(precision)\n",
    "    recall_per_class.append(recall)\n",
    "\n",
    "    print(f\"Classe {class_names[i]} :\")\n",
    "    print(f\"  Précision : {precision:.2f}\")\n",
    "    print(f\"  Rappel : {recall:.2f}\")\n",
    "    print(\" \")\n",
    "\n",
    "x = np.arange(nb_class)  # l'emplacement des classes\n",
    "width = 0.35  # largeur des barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, precision_per_class, width, label='Précision', color='grey')\n",
    "bars2 = ax.bar(x + width/2, recall_per_class, width, label='Rappel', color='black')\n",
    "\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Précision et Rappel par Classe')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend()\n",
    "\n",
    "# Afficher les scores au-dessus des barres\n",
    "for bar in bars1:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb15b7-739a-4558-9370-8e074d0a99f9",
   "metadata": {},
   "source": [
    "Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3997b-2a69-49fd-8f6d-68f24ef9d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for class_index in range(nb_class):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test_true == class_index, y_test_pred_prob[:, class_index])\n",
    "    average_precision = average_precision_score(y_test_true == class_index, y_test_pred_prob[:, class_index])\n",
    "    \n",
    "    plt.plot(recall, precision, marker='.', label=f'Classe {class_names[class_index]} (AP = {average_precision:.2f})')\n",
    "\n",
    "plt.xlabel('Rappel')\n",
    "plt.ylabel('Précision')\n",
    "plt.title('Courbe Précision-Rappel pour toutes les classes')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29f68b-8510-4af5-987c-317c3a9851b0",
   "metadata": {},
   "source": [
    "ROC curves et AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd6fa9-bfdf-4a19-9d1b-c2e37ee8b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(nb_class):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_true, y_test_pred_prob[:, i], pos_label=i)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve pour {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Ajouter des éléments au graphique\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # ligne diagonale\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC par classe')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ba5f6-5c3d-4896-8b65-33cce76994d7",
   "metadata": {},
   "source": [
    "Score F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcbbe7-91c0-49e6-abee-43478fc30601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du F1 score\n",
    "f1_per_class = f1_score(y_test_true, y_test_pred_classes, average=None)  # F1 score pour chaque classe\n",
    "f1_weighted = f1_score(y_test_true, y_test_pred_classes, average='weighted')  # F1 score global\n",
    "\n",
    "# Affichage des résultats\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"F1 Score pour {class_name}: {f1_per_class[i]:.2f}\")\n",
    "\n",
    "print(f\"F1 Score global (weighted): {f1_weighted:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_names, f1_per_class, color='skyblue', label='F1 Score par Classe')\n",
    "plt.axhline(y=f1_weighted, color='orange', linestyle='--', label=f\"F1 Score Global (weighted) : {f1_weighted:.2f}\")\n",
    "\n",
    "# Ajouter des annotations\n",
    "for bar, score in zip(bars, f1_per_class):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.05, f\"{score:.2f}\", ha='center', va='bottom', color='black')\n",
    "\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Histogramme des F1 Scores par Classe et F1 Score Global\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Score F1 est entre 0 et 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
